{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa50bd8",
   "metadata": {},
   "source": [
    "<H3>Install libs<H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4233ff37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mtl/DATA 6TB8/PROJECT AI/RAG-NLP/.venv/bin/pip: 2: exec: /media/mtl/DATA 6TB/PROJECT AI/RAG-NLP/.venv/bin/python: not found\n"
     ]
    }
   ],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán ch√≠nh cho m√¥ h√¨nh ng√¥n ng·ªØ, embedding, RAG v√† giao di·ªán web\n",
    "!pip install -q \\\n",
    "  \"torch>=2.0.0\" \\\n",
    "  \"transformers>=4.40.0\" \\\n",
    "  \"accelerate>=0.30.0\" \\\n",
    "  \"huggingface-hub>=0.23.0\" \\\n",
    "  \"sentence-transformers>=2.7.0\" \\\n",
    "  \"langchain>=0.2.0\" \\\n",
    "  \"langchain-core>=0.2.0\" \\\n",
    "  \"langchain-community>=0.1.0\" \\\n",
    "  \"langchain-text-splitters>=0.2.0\" \\\n",
    "  \"chromadb>=0.5.0\" \\\n",
    "  \"langchain-chroma>=0.2.0\" \\\n",
    "  \"pypdf>=4.2.0\" \\\n",
    "  \"gradio>=5.0.0\" \\\n",
    "  \"langchain-huggingface\" \\\n",
    "  \"wget\" \\\n",
    "  \"tqdm\" \\\n",
    "  \"ipywidgets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153d55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbe7be",
   "metadata": {},
   "source": [
    "<H3>Setup project + t·∫°o c·∫•u tr√∫c th∆∞ m·ª•c</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1e955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PROJECT_ROOT: /media/mtl/DATA 6TB8/PROJECT AI/RAG-NLP/rag_langchain\n",
      "‚úÖ Copy PDF v√†o: /media/mtl/DATA 6TB8/PROJECT AI/RAG-NLP/rag_langchain/data_source/generative_ai\n",
      "‚úÖ (Optional) Copy PDF kh√°c v√†o: /media/mtl/DATA 6TB8/PROJECT AI/RAG-NLP/rag_langchain/data_source/custom\n",
      "‚úÖ Chroma DB l∆∞u ·ªü: /media/mtl/DATA 6TB8/PROJECT AI/RAG-NLP/rag_langchain/chroma_data\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Root d·ª± √°n: d√πng folder \"rag_langchain\" n·∫±m c√πng c·∫•p notebook (nh∆∞ ·∫£nh b·∫°n)\n",
    "PROJECT_ROOT = os.path.abspath(\"rag_langchain\")\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data_source\", \"generative_ai\")  # b·∫°n copy PDF v√†o ƒë√¢y\n",
    "CUSTOM_DIR = os.path.join(PROJECT_ROOT, \"data_source\", \"custom\")       # tu·ª≥ ch·ªçn\n",
    "CHROMA_DIR = os.path.join(PROJECT_ROOT, \"chroma_data\")                 # l∆∞u vector DB\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CUSTOM_DIR, exist_ok=True)\n",
    "os.makedirs(CHROMA_DIR, exist_ok=True)\n",
    "\n",
    "# src (tu·ª≥ ch·ªçn, cho ƒë√∫ng c·∫•u tr√∫c t√†i li·ªáu)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, \"src\", \"base\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, \"src\", \"rag\"), exist_ok=True)\n",
    "\n",
    "# t·∫°o __init__.py\n",
    "for p in [\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"__init__.py\"),\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"base\", \"__init__.py\"),\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"rag\", \"__init__.py\"),\n",
    "]:\n",
    "    if not os.path.exists(p):\n",
    "        open(p, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "# th√™m PROJECT_ROOT v√†o sys.path (ph√≤ng khi t√°ch code)\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"‚úÖ PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"‚úÖ Copy PDF v√†o:\", DATA_DIR)\n",
    "print(\"‚úÖ (Optional) Copy PDF kh√°c v√†o:\", CUSTOM_DIR)\n",
    "print(\"‚úÖ Chroma DB l∆∞u ·ªü:\", CHROMA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496a4f6",
   "metadata": {},
   "source": [
    "<h3>Check d·ªØ li·ªáu PDF ƒë√£ c√≥ ch∆∞a</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30cacaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ S·ªë PDF trong generative_ai: 8\n",
      " - 05-vbhn-bgtvt.pdf\n",
      " - 168-nd-cp.signed.pdf\n",
      " - 2023_1335 + 1336_12-VBHN-BGTVT.pdf\n",
      " - 35-2024-qh15.pdf\n",
      " - 36-2024-qh15.pdf\n",
      " - 36-2024-qh15_tiep.pdf\n",
      " - 51-bgtvt-kem.pdf\n",
      " - 73-bca.pdf\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "pdf_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
    "print(\"üìÑ S·ªë PDF trong generative_ai:\", len(pdf_files))\n",
    "for f in pdf_files[:20]:\n",
    "    print(\" -\", os.path.basename(f))\n",
    "\n",
    "if len(pdf_files) == 0:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Ch∆∞a c√≥ PDF!\\n\"\n",
    "        f\"H√£y copy v√†i file .pdf v√†o folder:\\n{DATA_DIR}\\n\"\n",
    "        \"R·ªìi ch·∫°y l·∫°i cell n√†y.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917e761",
   "metadata": {},
   "source": [
    "<H3>Clean text + Loader + Chunking</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d456075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def clean_vietnamese_text(text: str) -> str:\n",
    "    # Chu·∫©n h√≥a Unicode ti·∫øng Vi·ªát\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    # Lo·∫°i b·ªè k√Ω t·ª± control (gi·ªØ \\n \\t)\n",
    "    text = \"\".join(\n",
    "        ch for ch in text\n",
    "        if (not unicodedata.category(ch).startswith(\"C\")) or ch in \"\\n\\t\"\n",
    "    )\n",
    "\n",
    "    # G·ªôp kho·∫£ng tr·∫Øng th·ª´a\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "class SimpleLoader:\n",
    "    def load_pdf(self, pdf_file: str):\n",
    "        docs = PyPDFLoader(pdf_file, extract_images=True).load()\n",
    "        for doc in docs:\n",
    "            doc.page_content = clean_vietnamese_text(doc.page_content)\n",
    "            # th√™m metadata ƒë·ªÉ debug (file name + page)\n",
    "            doc.metadata[\"source_file\"] = os.path.basename(pdf_file)\n",
    "        return docs\n",
    "\n",
    "    def load_dir(self, dir_path: str) -> List:\n",
    "        pdfs = sorted(glob.glob(os.path.join(dir_path, \"*.pdf\")))\n",
    "        if not pdfs:\n",
    "            raise ValueError(f\"No PDF files found in: {dir_path}\")\n",
    "\n",
    "        all_docs = []\n",
    "        for pdf in tqdm(pdfs, desc=\"Loading PDFs\"):\n",
    "            try:\n",
    "                all_docs.extend(self.load_pdf(pdf))\n",
    "            except Exception as e:\n",
    "                print(\"Skip:\", pdf, \"|\", e)\n",
    "        return all_docs\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self, chunk_size: int = 400, chunk_overlap: int = 120):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "    def split(self, documents):\n",
    "        return self.splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adcf0c",
   "metadata": {},
   "source": [
    "<H3>Vector DB (Chroma + Embeddings) - H·ªó tr·ª£ ‚Äúth√™m t√†i li·ªáu‚Äù (incremental)</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8474f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(\n",
    "        self,\n",
    "        documents=None,\n",
    "        # embedding_model: str = \"dangvantuan/vietnamese-document-embedding\",\n",
    "        embedding_model: str = \"keepitreal/vietnamese-sbert\",\n",
    "        collection_name: str = \"vietnamese_docs\",\n",
    "        persist_dir: str = None,\n",
    "    ):\n",
    "        self.persist_dir = persist_dir or CHROMA_DIR\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        # ‚úÖ B·∫≠t trust_remote_code cho model c√≥ custom code\n",
    "        self.embedding = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model,\n",
    "            model_kwargs={\"trust_remote_code\": True},\n",
    "        )\n",
    "\n",
    "        self.db = Chroma(\n",
    "            collection_name=self.collection_name,\n",
    "            embedding_function=self.embedding,\n",
    "            persist_directory=self.persist_dir,\n",
    "        )\n",
    "\n",
    "        if documents and len(documents) > 0:\n",
    "            self.add_documents(documents)\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        self.db.add_documents(documents)\n",
    "        if hasattr(self.db, \"persist\"):\n",
    "            try:\n",
    "                self.db.persist()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # ‚úÖ MMR ƒë·ªÉ b·ªõt tr√πng ƒëo·∫°n + ph·ªß √Ω r·ªông h∆°n (c√≥ filter theo file n·∫øu c·∫ßn)\n",
    "    def get_retriever(self, k: int = 8, source_file: Optional[str] = None):\n",
    "        search_kwargs = {\"k\": k, \"fetch_k\": max(20, k * 4), \"lambda_mult\": 0.2}\n",
    "        if source_file:\n",
    "            search_kwargs[\"filter\"] = {\"source_file\": source_file}\n",
    "        return self.db.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs=search_kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee973c9",
   "metadata": {},
   "source": [
    "<H3>LLM (Qwen) + fallback model nh·ªè cho m√°y y·∫øu</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93746b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "def get_hf_llm(\n",
    "    model_name: str = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    temperature: float = 0.2, # th√¢ÃÅp thiÃÄ ƒë√¥Ãâ lan man = chiÃÅnh xaÃÅc h∆°n\n",
    "    max_new_tokens: int = 900,\n",
    "):\n",
    "    # N·∫øu m√°y y·∫øu / kh√¥ng GPU -> d√πng model nh·ªè cho ch·∫Øc\n",
    "    if not torch.cuda.is_available() and model_name == \"Qwen/Qwen2.5-3B-Instruct\":\n",
    "        model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng th·∫•y GPU -> auto d√πng model nh·ªè:\", model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        temperature=0.1,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        return_full_text=False,   # ‚úÖ c·ª±c quan tr·ªçng\n",
    "        top_p=0.75,\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=gen_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdac6e7",
   "metadata": {},
   "source": [
    "<H3>Prompt + Parser + RAG chain (k√®m hi·ªÉn th·ªã context)</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32de1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def looks_like_toc_or_listing(text: str) -> bool:\n",
    "    t = (text or '').strip()\n",
    "    up = t.upper()\n",
    "    if 'M·ª§C L·ª§C' in up or 'DANH M·ª§C' in up:\n",
    "        return True\n",
    "    # nhi·ªÅu m√£/chu·ªói li·ªát k√™ ki·ªÉu QCVN, 8607.xx, qu√° nhi·ªÅu d·∫•u ph·∫©y\n",
    "    if len(re.findall(r'\\bQCVN\\b', up)) >= 3:\n",
    "        return True\n",
    "    if t.count(',') > 25:\n",
    "        return True\n",
    "    if len(re.findall(r'\\b\\d{4}/\\d{4}\\b', t)) >= 6:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "import re\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class FocusedAnswerParser(StrOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        text = (text or \"\").strip()\n",
    "\n",
    "        m = re.search(r\"\\[?\\s*TR·∫¢\\s*L·ªúI\\s*\\]?\\s*[:Ôºö]\\s*\", text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            text = text[m.end():].strip()\n",
    "\n",
    "        text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "        text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "        return text\n",
    "\n",
    "class OfflineRAG:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = PromptTemplate.from_template(\"\"\"\n",
    "B·∫°n l√† tr·ª£ l√Ω h·ªèi ƒë√°p ph√°p lu·∫≠t giao th√¥ng.\n",
    "CH·ªà ƒë∆∞·ª£c d√πng th√¥ng tin trong CONTEXT. Kh√¥ng b·ªãa.\n",
    "N·∫øu CONTEXT kh√¥ng ƒë·ªß, n√≥i: \"Ch∆∞a ƒë·ªß th√¥ng tin trong t√†i li·ªáu ƒë√£ n·∫°p\" v√† n√™u c·∫ßn th√™m g√¨.\n",
    "\n",
    "Tr·∫£ l·ªùi t·ªëi ƒëa 8 d√≤ng, g·∫°ch ƒë·∫ßu d√≤ng 2-4 √Ω, cu·ªëi c√¢u c√≥ ngu·ªìn (ten_file | page X).\n",
    "TUY·ªÜT ƒê·ªêI KH√îNG in ra nh√£n [M·∫™U TR·∫¢ L·ªúI], [M·ªû B√ÄI] hay h∆∞·ªõng d·∫´n n·ªôi b·ªô.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "C√ÇU H·ªéI: {question}\n",
    "\n",
    "TR·∫¢ L·ªúI:\n",
    "\"\"\".strip())\n",
    "        self.answer_parser = FocusedAnswerParser()\n",
    "\n",
    "    def get_chain(self, retriever):\n",
    "        def format_docs(docs):\n",
    "            blocks = []\n",
    "            seen = set()\n",
    "            for d in docs:\n",
    "                content = (d.page_content or \"\").strip()\n",
    "                if looks_like_toc_or_listing(content):\n",
    "                    continue\n",
    "                if len(content) < 40:\n",
    "                    continue\n",
    "                src = d.metadata.get(\"source_file\", \"unknown\")\n",
    "                page = d.metadata.get(\"page\", \"?\")\n",
    "\n",
    "                key = (src, page, hash(content))\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "\n",
    "                blocks.append(f\"[{src} | page {page}]\\n{content}\")\n",
    "            return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "        return (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | self.answer_parser\n",
    "        )\n",
    "\n",
    "    def get_context_only(self, retriever):\n",
    "        def format_docs(docs):\n",
    "            blocks = []\n",
    "            for d in docs:\n",
    "                src = d.metadata.get(\"source_file\", \"unknown\")\n",
    "                page = d.metadata.get(\"page\", \"?\")\n",
    "                blocks.append(f\"[{src} | page {page}]\\n{(d.page_content or '').strip()}\")\n",
    "            return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "        return retriever | format_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f5aa1",
   "metadata": {},
   "source": [
    "<H3>Build pipeline: load ‚Üí chunk ‚Üí vector ‚Üí chain</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1d89c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b43647e10543df9dbc2f32794b6872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Loading PDFs:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:02<00:03,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip: /media/mtl/DATA 6TB8/PROJECT AI/RAG-NLP/rag_langchain/data_source/generative_ai/168-nd-cp.signed.pdf | cannot reshape array of size 41654 into shape (2357,1670,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip: /media/mtl/DATA 6TB8/PROJECT AI/RAG-NLP/rag_langchain/data_source/generative_ai/51-bgtvt-kem.pdf | cannot reshape array of size 2819 into shape (93,97,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready. PDFs in kho: 8\n"
     ]
    }
   ],
   "source": [
    "# 1) Loader + Splitter\n",
    "loader = SimpleLoader()\n",
    "splitter = TextSplitter(chunk_size=400, chunk_overlap=120)\n",
    "\n",
    "# 2) VectorDB (persist)\n",
    "vdb = VectorDB(documents=None)  # m·ªü DB t·ª´ persist_dir\n",
    "\n",
    "# N·∫øu mu·ªën rebuild s·∫°ch DB m·ªói l·∫ßn ch·∫°y notebook, uncomment block d∆∞·ªõi:\n",
    "# import shutil\n",
    "# if os.path.exists(CHROMA_DIR):\n",
    "#     shutil.rmtree(CHROMA_DIR, ignore_errors=True)\n",
    "# vdb = VectorDB(documents=None)\n",
    "\n",
    "# 3) LLM + RAG\n",
    "llm = get_hf_llm()\n",
    "rag = OfflineRAG(llm)\n",
    "\n",
    "# 4) N·∫øu trong folder ƒë√£ c√≥ PDF th√¨ ingest 1 l·∫ßn (ƒë·ªÉ c√≥ d·ªØ li·ªáu)\n",
    "pdfs = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
    "if len(pdfs) > 0:\n",
    "    raw_docs = loader.load_dir(DATA_DIR)\n",
    "    split_docs = splitter.split(raw_docs)\n",
    "    vdb.add_documents(split_docs)\n",
    "\n",
    "retriever = vdb.get_retriever(k=10)\n",
    "rag_chain = rag.get_chain(retriever)\n",
    "ctx_chain = rag.get_context_only(retriever)\n",
    "\n",
    "def answer_question(question: str) -> str:\n",
    "    try:\n",
    "        return rag_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_context(question: str) -> str:\n",
    "    try:\n",
    "        return ctx_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Ready. PDFs in kho:\", len(glob.glob(os.path.join(DATA_DIR, '*.pdf'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3097de",
   "metadata": {},
   "source": [
    "<h3>‚Äúingest pipeline‚Äù cho file upload (copy ‚Üí load ‚Üí chunk ‚Üí add ‚Üí refresh chain)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b3b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def list_pdfs_md(data_dir: str) -> str:\n",
    "    pdfs = sorted(glob.glob(os.path.join(data_dir, \"*.pdf\")))\n",
    "    if not pdfs:\n",
    "        return \"*(Ch∆∞a c√≥ file PDF n√†o trong kho)*\"\n",
    "    lines = [f\"- {os.path.basename(p)}\" for p in pdfs]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def ingest_uploaded_pdfs(uploaded_files, loader, splitter, vdb, data_dir: str):\n",
    "    \"\"\"\n",
    "    uploaded_files: list[gradio UploadedFile] ho·∫∑c list[path]\n",
    "    - copy v√†o data_dir\n",
    "    - load -> chunk\n",
    "    - add v√†o Chroma\n",
    "    \"\"\"\n",
    "    if not uploaded_files:\n",
    "        return \"‚ùå B·∫°n ch∆∞a ch·ªçn file PDF n√†o.\"\n",
    "\n",
    "    saved = []\n",
    "    for f in uploaded_files:\n",
    "        # gr.File c√≥ th·ªÉ tr·∫£ object c√≥ thu·ªôc t√≠nh .name ho·∫∑c l√† string path\n",
    "        src_path = getattr(f, \"name\", None) or str(f)\n",
    "        base = os.path.basename(src_path)\n",
    "        dst_path = os.path.join(data_dir, base)\n",
    "\n",
    "        # copy v√†o kho\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        saved.append(dst_path)\n",
    "\n",
    "    # load + chunk ch·ªâ nh·ªØng file m·ªõi\n",
    "    new_docs = []\n",
    "    for p in saved:\n",
    "        try:\n",
    "            new_docs.extend(loader.load_pdf(p))\n",
    "        except Exception as e:\n",
    "            print(\"Skip load:\", p, e)\n",
    "\n",
    "    if not new_docs:\n",
    "        return \"‚ö†Ô∏è Copy xong nh∆∞ng kh√¥ng ƒë·ªçc ƒë∆∞·ª£c PDF (c√≥ th·ªÉ PDF scan ·∫£nh / l·ªói ƒë·ªãnh d·∫°ng).\"\n",
    "\n",
    "    new_chunks = splitter.split(new_docs)\n",
    "    if not new_chunks:\n",
    "        return \"‚ö†Ô∏è ƒê·ªçc ƒë∆∞·ª£c nh∆∞ng chunk r·ªóng (PDF c√≥ th·ªÉ to√†n ·∫£nh).\"\n",
    "\n",
    "    # add v√†o Vector DB\n",
    "    vdb.add_documents(new_chunks)\n",
    "\n",
    "    return f\"‚úÖ ƒê√£ n·∫°p {len(saved)} file | pages loaded: {len(new_docs)} | chunks added: {len(new_chunks)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffce31",
   "metadata": {},
   "source": [
    "<H3>Gradio UI</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356620f",
   "metadata": {},
   "source": [
    "<H4>Ch·ªëng tr√πng ingest b·∫±ng HASH + ch·ªâ ingest file m·ªõi<H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e692680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, hashlib, shutil, glob\n",
    "from datetime import datetime\n",
    "\n",
    "HASH_DB_PATH = os.path.join(PROJECT_ROOT, \"ingested_hashes.json\")\n",
    "\n",
    "def load_hash_db():\n",
    "    if os.path.exists(HASH_DB_PATH):\n",
    "        try:\n",
    "            with open(HASH_DB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_hash_db(db):\n",
    "    with open(HASH_DB_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(db, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def sha256_file(path, chunk_size=1024*1024):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "HASH_DB = load_hash_db()\n",
    "\n",
    "def ingest_uploaded_pdfs_no_dup(uploaded_files, loader, splitter, vdb, data_dir: str):\n",
    "    \"\"\"\n",
    "    - copy file v√†o kho\n",
    "    - t√≠nh hash\n",
    "    - n·∫øu hash ƒë√£ c√≥ => skip\n",
    "    - n·∫øu m·ªõi => load->chunk->add_documents\n",
    "    \"\"\"\n",
    "    if not uploaded_files:\n",
    "        return \"‚ùå B·∫°n ch∆∞a ch·ªçn file PDF n√†o.\"\n",
    "\n",
    "    added_files, skipped_files = [], []\n",
    "    new_docs_total, new_chunks_total = 0, 0\n",
    "\n",
    "    for f in uploaded_files:\n",
    "        src_path = getattr(f, \"name\", None) or str(f)\n",
    "        base = os.path.basename(src_path)\n",
    "        dst_path = os.path.join(data_dir, base)\n",
    "\n",
    "        # copy v√†o kho (n·∫øu tr√πng t√™n th√¨ th√™m timestamp)\n",
    "        if os.path.exists(dst_path):\n",
    "            name, ext = os.path.splitext(base)\n",
    "            dst_path = os.path.join(data_dir, f\"{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}{ext}\")\n",
    "\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "        file_hash = sha256_file(dst_path)\n",
    "\n",
    "        if file_hash in HASH_DB:\n",
    "            skipped_files.append(os.path.basename(dst_path))\n",
    "            # file tr√πng hash th√¨ x√≥a b·∫£n copy v·ª´a t·∫°o ƒë·ªÉ kho s·∫°ch\n",
    "            try:\n",
    "                os.remove(dst_path)\n",
    "            except:\n",
    "                pass\n",
    "            continue\n",
    "\n",
    "        # file m·ªõi\n",
    "        try:\n",
    "            docs = loader.load_pdf(dst_path)\n",
    "        except Exception as e:\n",
    "            # n·∫øu kh√¥ng ƒë·ªçc ƒë∆∞·ª£c th√¨ b·ªè v√† x√≥a\n",
    "            try: os.remove(dst_path)\n",
    "            except: pass\n",
    "            continue\n",
    "\n",
    "        chunks = splitter.split(docs)\n",
    "        if chunks:\n",
    "            vdb.add_documents(chunks)\n",
    "            HASH_DB[file_hash] = {\n",
    "                \"file\": os.path.basename(dst_path),\n",
    "                \"added_at\": datetime.now().isoformat()\n",
    "            }\n",
    "            added_files.append(os.path.basename(dst_path))\n",
    "            new_docs_total += len(docs)\n",
    "            new_chunks_total += len(chunks)\n",
    "        else:\n",
    "            # chunk r·ªóng th√¨ b·ªè\n",
    "            try: os.remove(dst_path)\n",
    "            except: pass\n",
    "\n",
    "    save_hash_db(HASH_DB)\n",
    "\n",
    "    msg = f\"‚úÖ Added: {len(added_files)} file(s), pages: {new_docs_total}, chunks: {new_chunks_total}\\n\"\n",
    "    if added_files:\n",
    "        msg += \"  + \" + \", \".join(added_files) + \"\\n\"\n",
    "    if skipped_files:\n",
    "        msg += f\"‚ö†Ô∏è Skipped (duplicate by hash): {len(skipped_files)}\\n\"\n",
    "        msg += \"  - \" + \", \".join(skipped_files)\n",
    "\n",
    "    return msg.strip()\n",
    "\n",
    "def list_pdf_names(data_dir):\n",
    "    return [os.path.basename(p) for p in sorted(glob.glob(os.path.join(data_dir, \"*.pdf\")))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302ed06",
   "metadata": {},
   "source": [
    "<H4>Top-k k√®m similarity score (Chroma)</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf00782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_scores(query: str, k: int = 4):\n",
    "    \"\"\"\n",
    "    Tr·∫£ v·ªÅ (doc, score). V·ªõi Chroma: score th∆∞·ªùng l√† distance (th·∫•p h∆°n = gi·ªëng h∆°n)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pairs = vdb.db.similarity_search_with_score(query, k=k)\n",
    "        return pairs\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def format_scored_context(pairs):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for doc, score in pairs:\n",
    "        src = doc.metadata.get(\"source_file\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        text = (doc.page_content or \"\").strip()\n",
    "\n",
    "        key = (src, page, text[:200])  # ƒë·ªß ƒë·ªÉ tr√°nh tr√πng\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "\n",
    "        out.append(f\"[{src} | page {page} | score={score:.4f}]\\n{text}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def query_has_doc_id(q: str):\n",
    "    # b·∫Øt d·∫°ng 12/2022 ho·∫∑c 12/2022/TT-BGTVT\n",
    "    m = re.search(r'\\b\\d{1,3}/\\d{4}\\b', q or '')\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def answer_with_scored_ctx(q: str, selected_pdf: str):\n",
    "    # 1) retriever theo file ƒë√£ ch·ªçn\n",
    "    local_retriever = vdb.get_retriever(k=6, source_file=selected_pdf)\n",
    "\n",
    "    # 2) l·∫•y context + score ƒë·ªÉ debug\n",
    "    try:\n",
    "        pairs = vdb.db.similarity_search_with_score(q, k=6, filter={\"source_file\": selected_pdf})\n",
    "    except TypeError:\n",
    "        # fallback n·∫øu vector store kh√¥ng h·ªó tr·ª£ filter ·ªü ƒë√¢y\n",
    "        pairs = vdb.db.similarity_search_with_score(q, k=6)\n",
    "\n",
    "    ctx = format_scored_context(pairs)\n",
    "\n",
    "    # 3) grounding check: n·∫øu h·ªèi s·ªë vƒÉn b·∫£n m√† trong context kh√¥ng c√≥ -> tr·∫£ v·ªÅ r√µ r√†ng\n",
    "    doc_id = query_has_doc_id(q)\n",
    "    if doc_id:\n",
    "        joined = '\\n'.join([(d.page_content or '') for d, _ in pairs]).lower()\n",
    "        if doc_id.lower() not in joined:\n",
    "            ans = (\n",
    "                f\"Kh√¥ng t√¨m th·∫•y c·ª•m `{doc_id}` trong **{selected_pdf}** t·ª´ c√°c ƒëo·∫°n tr√≠ch hi·ªán c√≥.\\n\"\n",
    "                f\"G·ª£i √Ω: b·∫°n h√£y **upload ƒë√∫ng PDF c·ªßa Th√¥ng t∆∞ {doc_id}** (ho·∫∑c vƒÉn b·∫£n h·ª£p nh·∫•t ƒë√∫ng s·ªë), \"\n",
    "                \"r·ªìi h·ªèi l·∫°i ƒë·ªÉ m√¨nh tr·∫£ l·ªùi chu·∫©n theo ƒêi·ªÅu/Kho·∫£n.\"\n",
    "            )\n",
    "            return ans, ctx\n",
    "\n",
    "    # 4) g·ªçi RAG chain theo retriever ƒë√£ l·ªçc\n",
    "    local_chain = rag.get_chain(local_retriever)\n",
    "    ans = local_chain.invoke(q)\n",
    "    return ans, ctx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422330d0",
   "metadata": {},
   "source": [
    "<H4>UI: click PDF ƒë·ªÉ ‚Äúm·ªü/t·∫£i‚Äù + upload ingest ch·ªëng tr√πng + context c√≥ score</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc729a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://bc6d1cf5847c729c7d.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bc6d1cf5847c729c7d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# ‚úÖ L∆∞u √Ω: trong Gradio, ph·∫£i t·∫°o component xong r·ªìi m·ªõi bind event (btn.click).\n",
    "# N·∫øu bind tr∆∞·ªõc khi pdf_picker ƒë∆∞·ª£c t·∫°o, s·∫Ω d√≠nh NameError.\n",
    "\n",
    "def ui_refresh_choices():\n",
    "    names = list_pdf_names(DATA_DIR)\n",
    "    return gr.update(choices=names, value=names[0] if names else None)\n",
    "\n",
    "def ui_open_file(selected_name):\n",
    "    if not selected_name:\n",
    "        return None\n",
    "    path = os.path.join(DATA_DIR, selected_name)\n",
    "    return path if os.path.exists(path) else None\n",
    "\n",
    "def ui_upload_and_ingest(files):\n",
    "    msg = ingest_uploaded_pdfs_no_dup(files, loader, splitter, vdb, DATA_DIR)\n",
    "\n",
    "    # refresh chain (ƒë·ªÉ ch·∫Øc k√®o)\n",
    "    global retriever, rag_chain, ctx_chain\n",
    "    retriever = vdb.get_retriever(k=4)\n",
    "    rag_chain = rag.get_chain(retriever)\n",
    "    ctx_chain = rag.get_context_only(retriever)\n",
    "\n",
    "    # refresh list & open first file\n",
    "    names = list_pdf_names(DATA_DIR)\n",
    "    first = names[0] if names else None\n",
    "    return msg, ui_refresh_choices(), ui_open_file(first)\n",
    "\n",
    "METHOD_MD = \"\"\"\n",
    "## üß† Ph∆∞∆°ng ph√°p & c√°ch th·ª©c ho·∫°t ƒë·ªông (Demo RAG)\n",
    "\n",
    "### A. Ingest (N·∫°p PDF)\n",
    "1) Upload PDF ‚Üí copy v√†o kho (`data_source/generative_ai`)\n",
    "2) T√≠nh **SHA256 hash** ‚Üí n·∫øu ƒë√£ c√≥ hash th√¨ **skip** (ch·ªëng tr√πng)\n",
    "3) Tr√≠ch xu·∫•t text t·ª´ PDF (PyPDFLoader)\n",
    "4) L√†m s·∫°ch ti·∫øng Vi·ªát + chunking (chunk_size=400, overlap=120)\n",
    "5) Embedding ‚Üí l∆∞u v√†o ChromaDB (persist)\n",
    "\n",
    "### B. Retrieval + Answer (RAG)\n",
    "1) C√¢u h·ªèi ‚Üí truy v·∫•n Chroma **Top-k similarity**\n",
    "2) L·∫•y c√°c chunk li√™n quan + **score** (ƒë·ªÉ gi·∫£i th√≠ch v√¨ sao n√≥ ch·ªçn ƒëo·∫°n ƒë√≥)\n",
    "3) Nh√©t context v√†o prompt ‚Üí LLM sinh c√¢u tr·∫£ l·ªùi\n",
    "4) UI show context + score ƒë·ªÉ ch·ª©ng minh ‚Äúc√≥ d·∫´n ch·ª©ng‚Äù\n",
    "\"\"\"\n",
    "\n",
    "# chu·∫©n b·ªã danh s√°ch file hi·ªán c√≥ ƒë·ªÉ set default value cho picker + file view\n",
    "_names = list_pdf_names(DATA_DIR)\n",
    "_first = _names[0] if _names else None\n",
    "\n",
    "with gr.Blocks(title=\"RAG - H·ªá th·ªëng h·ªèi ƒë√°p lu·∫≠t giao th√¥ng\") as demo:\n",
    "    gr.Markdown(\"# üìå RAG ‚Äì H·ªÜ TH·ªêNG H·ªéI ƒê√ÅP LU·∫¨T GIAO TH√îNG\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # LEFT\n",
    "        with gr.Column(scale=2):\n",
    "            question = gr.Textbox(label=\"C√¢u h·ªèi\", placeholder=\"Nh·∫≠p c√¢u h·ªèi v·ªÅ n·ªôi dung trong PDF...\", lines=3)\n",
    "            btn = gr.Button(\"G·ª≠i\", variant=\"primary\")\n",
    "            answer = gr.Textbox(label=\"C√¢u tr·∫£ l·ªùi\", lines=14, interactive=False)\n",
    "\n",
    "            gr.Markdown(\"## üîé Context (Top-k chunks + similarity score)\")\n",
    "            context = gr.Textbox(label=\"Top-k Context (score)\", lines=12, interactive=False)\n",
    "\n",
    "        # RIGHT\n",
    "        with gr.Column(scale=1):\n",
    "            with gr.Tabs():\n",
    "                with gr.Tab(\"üìö T√†i li·ªáu & Upload\"):\n",
    "                    gr.Markdown(\"## üìö Danh s√°ch t√†i li·ªáu ƒë√£ n·∫°p\")\n",
    "\n",
    "                    pdf_picker = gr.Radio(\n",
    "                        choices=_names,\n",
    "                        label=\"Ch·ªçn t√†i li·ªáu\",\n",
    "                        value=_first\n",
    "                    )\n",
    "\n",
    "                    # ‚ÄúM·ªü/t·∫£i‚Äù file\n",
    "                    pdf_file = gr.File(label=\"M·ªü/T·∫£i PDF\", value=ui_open_file(_first))\n",
    "                    pdf_picker.change(fn=ui_open_file, inputs=pdf_picker, outputs=pdf_file)\n",
    "\n",
    "                    gr.Markdown(\"## ‚ûï N·∫°p th√™m PDF m·ªõi\")\n",
    "                    uploader = gr.File(label=\"Ch·ªçn file PDF\", file_types=[\".pdf\"], file_count=\"multiple\")\n",
    "                    ingest_btn = gr.Button(\"N·∫°p & c·∫≠p nh·∫≠t kho\", variant=\"secondary\")\n",
    "                    ingest_status = gr.Textbox(label=\"Tr·∫°ng th√°i\", lines=4, interactive=False)\n",
    "\n",
    "                    ingest_btn.click(\n",
    "                        fn=ui_upload_and_ingest,\n",
    "                        inputs=uploader,\n",
    "                        outputs=[ingest_status, pdf_picker, pdf_file]\n",
    "                    )\n",
    "\n",
    "                with gr.Tab(\"üß© Ph∆∞∆°ng ph√°p\"):\n",
    "                    gr.Markdown(METHOD_MD)\n",
    "\n",
    "    # ‚úÖ Bind event SAU KHI pdf_picker ƒë√£ ƒë∆∞·ª£c t·∫°o\n",
    "    btn.click(fn=answer_with_scored_ctx, inputs=[question, pdf_picker], outputs=[answer, context])\n",
    "\n",
    "demo.launch(share=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "patched": {
   "rag_fix": {
    "inserted_filter": true,
    "patched_answer": true,
    "patched_click": false,
    "patched_gen": true,
    "patched_retriever": true
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
