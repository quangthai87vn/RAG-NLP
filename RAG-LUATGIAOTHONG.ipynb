{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa50bd8",
   "metadata": {},
   "source": [
    "<H3>Install libs<H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4233ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán ch√≠nh cho m√¥ h√¨nh ng√¥n ng·ªØ, embedding, RAG v√† giao di·ªán web\n",
    "!pip install -q \\\n",
    "  \"torch>=2.0.0\" \\\n",
    "  \"transformers>=4.40.0\" \\\n",
    "  \"accelerate>=0.30.0\" \\\n",
    "  \"huggingface-hub>=0.23.0\" \\\n",
    "  \"sentence-transformers>=2.7.0\" \\\n",
    "  \"langchain>=0.2.0\" \\\n",
    "  \"langchain-core>=0.2.0\" \\\n",
    "  \"langchain-community>=0.1.0\" \\\n",
    "  \"langchain-text-splitters>=0.2.0\" \\\n",
    "  \"chromadb>=0.5.0\" \\\n",
    "  \"langchain-chroma>=0.2.0\" \\\n",
    "  \"pypdf>=4.2.0\" \\\n",
    "  \"gradio>=5.0.0\" \\\n",
    "  \"langchain-huggingface\" \\\n",
    "  \"wget\" \\\n",
    "  \"tqdm\" \\\n",
    "  \"ipywidgets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbe7be",
   "metadata": {},
   "source": [
    "<H3>Setup project + t·∫°o c·∫•u tr√∫c th∆∞ m·ª•c</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1e955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PROJECT_ROOT: /media/mtl/DATA 6TB/PROJECT AI/RAG-NLP/rag_langchain\n",
      "‚úÖ Copy PDF v√†o: /media/mtl/DATA 6TB/PROJECT AI/RAG-NLP/rag_langchain/data_source/generative_ai\n",
      "‚úÖ (Optional) Copy PDF kh√°c v√†o: /media/mtl/DATA 6TB/PROJECT AI/RAG-NLP/rag_langchain/data_source/custom\n",
      "‚úÖ Chroma DB l∆∞u ·ªü: /media/mtl/DATA 6TB/PROJECT AI/RAG-NLP/rag_langchain/chroma_data\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Root d·ª± √°n: d√πng folder \"rag_langchain\" n·∫±m c√πng c·∫•p notebook (nh∆∞ ·∫£nh b·∫°n)\n",
    "PROJECT_ROOT = os.path.abspath(\"rag_langchain\")\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data_source\", \"generative_ai\")  # b·∫°n copy PDF v√†o ƒë√¢y\n",
    "CUSTOM_DIR = os.path.join(PROJECT_ROOT, \"data_source\", \"custom\")       # tu·ª≥ ch·ªçn\n",
    "CHROMA_DIR = os.path.join(PROJECT_ROOT, \"chroma_data\")                 # l∆∞u vector DB\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CUSTOM_DIR, exist_ok=True)\n",
    "os.makedirs(CHROMA_DIR, exist_ok=True)\n",
    "\n",
    "# src (tu·ª≥ ch·ªçn, cho ƒë√∫ng c·∫•u tr√∫c t√†i li·ªáu)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, \"src\", \"base\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, \"src\", \"rag\"), exist_ok=True)\n",
    "\n",
    "# t·∫°o __init__.py\n",
    "for p in [\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"__init__.py\"),\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"base\", \"__init__.py\"),\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"rag\", \"__init__.py\"),\n",
    "]:\n",
    "    if not os.path.exists(p):\n",
    "        open(p, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "# th√™m PROJECT_ROOT v√†o sys.path (ph√≤ng khi t√°ch code)\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"‚úÖ PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"‚úÖ Copy PDF v√†o:\", DATA_DIR)\n",
    "print(\"‚úÖ (Optional) Copy PDF kh√°c v√†o:\", CUSTOM_DIR)\n",
    "print(\"‚úÖ Chroma DB l∆∞u ·ªü:\", CHROMA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496a4f6",
   "metadata": {},
   "source": [
    "<h3>Check d·ªØ li·ªáu PDF ƒë√£ c√≥ ch∆∞a</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30cacaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ S·ªë PDF trong generative_ai: 5\n",
      " - 168-nd-cp.signed.pdf\n",
      " - 336nd.signed.pdf\n",
      " - 35-2024-qh15.pdf\n",
      " - 36-2024-qh15.pdf\n",
      " - 36-2024-qh15_tiep.pdf\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "pdf_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
    "print(\"üìÑ S·ªë PDF trong generative_ai:\", len(pdf_files))\n",
    "for f in pdf_files[:20]:\n",
    "    print(\" -\", os.path.basename(f))\n",
    "\n",
    "if len(pdf_files) == 0:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Ch∆∞a c√≥ PDF!\\n\"\n",
    "        f\"H√£y copy v√†i file .pdf v√†o folder:\\n{DATA_DIR}\\n\"\n",
    "        \"R·ªìi ch·∫°y l·∫°i cell n√†y.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917e761",
   "metadata": {},
   "source": [
    "<H3>Clean text + Loader + Chunking</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d456075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def clean_vietnamese_text(text: str) -> str:\n",
    "    # Chu·∫©n h√≥a Unicode ti·∫øng Vi·ªát\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    # Lo·∫°i b·ªè k√Ω t·ª± control (gi·ªØ \\n \\t)\n",
    "    text = \"\".join(\n",
    "        ch for ch in text\n",
    "        if (not unicodedata.category(ch).startswith(\"C\")) or ch in \"\\n\\t\"\n",
    "    )\n",
    "\n",
    "    # G·ªôp kho·∫£ng tr·∫Øng th·ª´a\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "class SimpleLoader:\n",
    "    def load_pdf(self, pdf_file: str):\n",
    "        docs = PyPDFLoader(pdf_file, extract_images=True).load()\n",
    "        for doc in docs:\n",
    "            doc.page_content = clean_vietnamese_text(doc.page_content)\n",
    "            # th√™m metadata ƒë·ªÉ debug (file name + page)\n",
    "            doc.metadata[\"source_file\"] = os.path.basename(pdf_file)\n",
    "        return docs\n",
    "\n",
    "    def load_dir(self, dir_path: str) -> List:\n",
    "        pdfs = sorted(glob.glob(os.path.join(dir_path, \"*.pdf\")))\n",
    "        if not pdfs:\n",
    "            raise ValueError(f\"No PDF files found in: {dir_path}\")\n",
    "\n",
    "        all_docs = []\n",
    "        for pdf in tqdm(pdfs, desc=\"Loading PDFs\"):\n",
    "            try:\n",
    "                all_docs.extend(self.load_pdf(pdf))\n",
    "            except Exception as e:\n",
    "                print(\"Skip:\", pdf, \"|\", e)\n",
    "        return all_docs\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self, chunk_size: int = 400, chunk_overlap: int = 120):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "    def split(self, documents):\n",
    "        return self.splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adcf0c",
   "metadata": {},
   "source": [
    "<H3>Vector DB (Chroma + Embeddings) - H·ªó tr·ª£ ‚Äúth√™m t√†i li·ªáu‚Äù (incremental)</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8474f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(\n",
    "        self,\n",
    "        documents=None,\n",
    "        embedding_model: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        collection_name: str = \"vietnamese_docs\",\n",
    "        persist_dir: str = None,\n",
    "    ):\n",
    "        self.persist_dir = persist_dir or CHROMA_DIR\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        self.embedding = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "        # lu√¥n m·ªü collection persist ƒë·ªÉ d√πng l√¢u d√†i\n",
    "        self.db = Chroma(\n",
    "            collection_name=self.collection_name,\n",
    "            embedding_function=self.embedding,\n",
    "            persist_directory=self.persist_dir,\n",
    "        )\n",
    "\n",
    "        # n·∫øu c√≥ documents ban ƒë·∫ßu th√¨ add lu√¥n\n",
    "        if documents and len(documents) > 0:\n",
    "            self.add_documents(documents)\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        # Add incremental documents v√†o DB\n",
    "        self.db.add_documents(documents)\n",
    "\n",
    "        # Persist n·∫øu c√≥ h·ªó tr·ª£\n",
    "        if hasattr(self.db, \"persist\"):\n",
    "            try:\n",
    "                self.db.persist()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def get_retriever(self, k: int = 4):\n",
    "        return self.db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": k},\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee973c9",
   "metadata": {},
   "source": [
    "<H3>LLM (Qwen) + fallback model nh·ªè cho m√°y y·∫øu</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93746b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "def get_hf_llm(\n",
    "    model_name: str = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    temperature: float = 0.2,\n",
    "    max_new_tokens: int = 450,\n",
    "):\n",
    "    # N·∫øu m√°y y·∫øu / kh√¥ng GPU -> d√πng model nh·ªè cho ch·∫Øc\n",
    "    if not torch.cuda.is_available() and model_name == \"Qwen/Qwen2.5-3B-Instruct\":\n",
    "        model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng th·∫•y GPU -> auto d√πng model nh·ªè:\", model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_p=0.75,\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=gen_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdac6e7",
   "metadata": {},
   "source": [
    "<H3>Prompt + Parser + RAG chain (k√®m hi·ªÉn th·ªã context)</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32de1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class FocusedAnswerParser(StrOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        text = (text or \"\").strip()\n",
    "\n",
    "        if \"[TR·∫¢ L·ªúI]:\" in text:\n",
    "            answer = text.split(\"[TR·∫¢ L·ªúI]:\")[-1].strip()\n",
    "        else:\n",
    "            answer = text\n",
    "\n",
    "        answer = re.sub(r\"\\n+\", \" \", answer).strip()\n",
    "\n",
    "        # gi·ªõi h·∫°n 3-5 c√¢u\n",
    "        parts = [p.strip() for p in re.split(r\"(?<=[\\.\\!\\?])\\s+\", answer) if p.strip()]\n",
    "        if len(parts) > 5:\n",
    "            answer = \" \".join(parts[:5]) + \" ...\"\n",
    "        return answer\n",
    "\n",
    "class OfflineRAG:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = PromptTemplate.from_template(\"\"\"\n",
    "B·∫°n l√† tr·ª£ l√Ω AI ph√¢n t√≠ch t√†i li·ªáu ti·∫øng Vi·ªát.\n",
    "\n",
    "[T√ÄI LI·ªÜU]:\n",
    "{context}\n",
    "\n",
    "[C√ÇU H·ªéI]:\n",
    "{question}\n",
    "\n",
    "H√£y tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu. N·∫øu t√†i li·ªáu kh√¥ng c√≥ th√¥ng tin, n√≥i r√µ \"Kh√¥ng c√≥ th√¥ng tin\".\n",
    "Tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß th√¥ng tin (3-5 c√¢u chi ti·∫øt), kh√¥ng th√™m b·∫•t k·ª≥ chi ti·∫øt n√†o ngo√†i t√†i li·ªáu.\n",
    "\n",
    "[TR·∫¢ L·ªúI]:\n",
    "\"\"\".strip())\n",
    "        self.answer_parser = FocusedAnswerParser()\n",
    "\n",
    "    def get_chain(self, retriever):\n",
    "        def format_docs(docs):\n",
    "            # tr·∫£ v·ªÅ context + metadata cho demo\n",
    "            blocks = []\n",
    "            seen = set()\n",
    "            for d in docs:\n",
    "                content = (d.page_content or \"\").strip()\n",
    "                if not content or len(content) < 40:\n",
    "                    continue\n",
    "                key = content[:200]\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "                src = d.metadata.get(\"source_file\", \"unknown\")\n",
    "                page = d.metadata.get(\"page\", \"?\")\n",
    "                blocks.append(f\"[{src} | page {page}]\\n{content}\")\n",
    "            return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "        chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | self.answer_parser\n",
    "        )\n",
    "        return chain\n",
    "\n",
    "    def get_context_only(self, retriever):\n",
    "        # ti·ªán ƒë·ªÉ show context ri√™ng tr√™n UI\n",
    "        def format_docs(docs):\n",
    "            blocks = []\n",
    "            for d in docs:\n",
    "                src = d.metadata.get(\"source_file\", \"unknown\")\n",
    "                page = d.metadata.get(\"page\", \"?\")\n",
    "                blocks.append(f\"[{src} | page {page}]\\n{(d.page_content or '').strip()}\")\n",
    "            return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "        return retriever | format_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f5aa1",
   "metadata": {},
   "source": [
    "<H3>Build pipeline: load ‚Üí chunk ‚Üí vector ‚Üí chain</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf1d89c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db3a74f2f704654917246e6146be863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Loading PDFs:  20%|‚ñà‚ñà        | 1/5 [00:00<00:02,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip: /media/mtl/DATA 6TB/PROJECT AI/RAG-NLP/rag_langchain/data_source/generative_ai/168-nd-cp.signed.pdf | cannot reshape array of size 41654 into shape (2357,1670,newaxis)\n",
      "Skip: /media/mtl/DATA 6TB/PROJECT AI/RAG-NLP/rag_langchain/data_source/generative_ai/336nd.signed.pdf | cannot reshape array of size 37014 into shape (2361,1672,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:04<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready. PDFs in kho: 5\n"
     ]
    }
   ],
   "source": [
    "# 1) Loader + Splitter\n",
    "loader = SimpleLoader()\n",
    "splitter = TextSplitter(chunk_size=400, chunk_overlap=120)\n",
    "\n",
    "# 2) VectorDB (persist)\n",
    "vdb = VectorDB(documents=None)  # m·ªü DB t·ª´ persist_dir\n",
    "\n",
    "# N·∫øu mu·ªën rebuild s·∫°ch DB m·ªói l·∫ßn ch·∫°y notebook, uncomment block d∆∞·ªõi:\n",
    "# import shutil\n",
    "# if os.path.exists(CHROMA_DIR):\n",
    "#     shutil.rmtree(CHROMA_DIR, ignore_errors=True)\n",
    "# vdb = VectorDB(documents=None)\n",
    "\n",
    "# 3) LLM + RAG\n",
    "llm = get_hf_llm()\n",
    "rag = OfflineRAG(llm)\n",
    "\n",
    "# 4) N·∫øu trong folder ƒë√£ c√≥ PDF th√¨ ingest 1 l·∫ßn (ƒë·ªÉ c√≥ d·ªØ li·ªáu)\n",
    "pdfs = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
    "if len(pdfs) > 0:\n",
    "    raw_docs = loader.load_dir(DATA_DIR)\n",
    "    split_docs = splitter.split(raw_docs)\n",
    "    vdb.add_documents(split_docs)\n",
    "\n",
    "retriever = vdb.get_retriever(k=4)\n",
    "rag_chain = rag.get_chain(retriever)\n",
    "ctx_chain = rag.get_context_only(retriever)\n",
    "\n",
    "def answer_question(question: str) -> str:\n",
    "    try:\n",
    "        return rag_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_context(question: str) -> str:\n",
    "    try:\n",
    "        return ctx_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Ready. PDFs in kho:\", len(glob.glob(os.path.join(DATA_DIR, '*.pdf'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3097de",
   "metadata": {},
   "source": [
    "<h3>‚Äúingest pipeline‚Äù cho file upload (copy ‚Üí load ‚Üí chunk ‚Üí add ‚Üí refresh chain)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b3b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def list_pdfs_md(data_dir: str) -> str:\n",
    "    pdfs = sorted(glob.glob(os.path.join(data_dir, \"*.pdf\")))\n",
    "    if not pdfs:\n",
    "        return \"*(Ch∆∞a c√≥ file PDF n√†o trong kho)*\"\n",
    "    lines = [f\"- {os.path.basename(p)}\" for p in pdfs]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def ingest_uploaded_pdfs(uploaded_files, loader, splitter, vdb, data_dir: str):\n",
    "    \"\"\"\n",
    "    uploaded_files: list[gradio UploadedFile] ho·∫∑c list[path]\n",
    "    - copy v√†o data_dir\n",
    "    - load -> chunk\n",
    "    - add v√†o Chroma\n",
    "    \"\"\"\n",
    "    if not uploaded_files:\n",
    "        return \"‚ùå B·∫°n ch∆∞a ch·ªçn file PDF n√†o.\"\n",
    "\n",
    "    saved = []\n",
    "    for f in uploaded_files:\n",
    "        # gr.File c√≥ th·ªÉ tr·∫£ object c√≥ thu·ªôc t√≠nh .name ho·∫∑c l√† string path\n",
    "        src_path = getattr(f, \"name\", None) or str(f)\n",
    "        base = os.path.basename(src_path)\n",
    "        dst_path = os.path.join(data_dir, base)\n",
    "\n",
    "        # copy v√†o kho\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        saved.append(dst_path)\n",
    "\n",
    "    # load + chunk ch·ªâ nh·ªØng file m·ªõi\n",
    "    new_docs = []\n",
    "    for p in saved:\n",
    "        try:\n",
    "            new_docs.extend(loader.load_pdf(p))\n",
    "        except Exception as e:\n",
    "            print(\"Skip load:\", p, e)\n",
    "\n",
    "    if not new_docs:\n",
    "        return \"‚ö†Ô∏è Copy xong nh∆∞ng kh√¥ng ƒë·ªçc ƒë∆∞·ª£c PDF (c√≥ th·ªÉ PDF scan ·∫£nh / l·ªói ƒë·ªãnh d·∫°ng).\"\n",
    "\n",
    "    new_chunks = splitter.split(new_docs)\n",
    "    if not new_chunks:\n",
    "        return \"‚ö†Ô∏è ƒê·ªçc ƒë∆∞·ª£c nh∆∞ng chunk r·ªóng (PDF c√≥ th·ªÉ to√†n ·∫£nh).\"\n",
    "\n",
    "    # add v√†o Vector DB\n",
    "    vdb.add_documents(new_chunks)\n",
    "\n",
    "    return f\"‚úÖ ƒê√£ n·∫°p {len(saved)} file | pages loaded: {len(new_docs)} | chunks added: {len(new_chunks)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffce31",
   "metadata": {},
   "source": [
    "<H3>Gradio UI</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7457224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://8d1179b99ab439384b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8d1179b99ab439384b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def ui_refresh_pdf_list():\n",
    "    return list_pdfs_md(DATA_DIR)\n",
    "\n",
    "def ui_upload_and_ingest(files):\n",
    "    msg = ingest_uploaded_pdfs(files, loader, splitter, vdb, DATA_DIR)\n",
    "\n",
    "    # Sau khi add docs, retriever/chain v·∫´n d√πng ƒë∆∞·ª£c v√¨ n√≥ tr·ªè v√†o c√πng DB.\n",
    "    # Nh∆∞ng ƒë·ªÉ ch·∫Øc k√®o, refresh retriever + chains:\n",
    "    global retriever, rag_chain, ctx_chain\n",
    "    retriever = vdb.get_retriever(k=4)\n",
    "    rag_chain = rag.get_chain(retriever)\n",
    "    ctx_chain = rag.get_context_only(retriever)\n",
    "\n",
    "    return msg, ui_refresh_pdf_list()\n",
    "\n",
    "def qa_with_ctx(q):\n",
    "    return answer_question(q), get_context(q)\n",
    "\n",
    "with gr.Blocks(title=\"RAG: H√äÃ£ TH√îÃÅNG HOÃâI ƒêAÃÅP LU√ÇÃ£T GIAO TH√îNG\") as demo:\n",
    "    gr.Markdown(\"# üìå RAG ‚Äì H√äÃ£ TH√îÃÅNG HOÃâI ƒêAÃÅP LU√ÇÃ£T GIAO TH√îNG\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # LEFT: QA\n",
    "        with gr.Column(scale=2):\n",
    "            question = gr.Textbox(\n",
    "                label=\"C√¢u h·ªèi\",\n",
    "                placeholder=\"Nh·∫≠p c√¢u h·ªèi v·ªÅ n·ªôi dung trong PDF...\",\n",
    "                lines=3\n",
    "            )\n",
    "            btn = gr.Button(\"G·ª≠i\", variant=\"primary\")\n",
    "            answer = gr.Textbox(label=\"C√¢u tr·∫£ l·ªùi\", lines=6, interactive=False)\n",
    "\n",
    "            gr.Markdown(\"## üîé Context (Top-k chunks h·ªá th·ªëng l·∫•y ra)\")\n",
    "            context = gr.Textbox(label=\"Top-k Context\", lines=10, interactive=False)\n",
    "\n",
    "            btn.click(fn=qa_with_ctx, inputs=question, outputs=[answer, context])\n",
    "\n",
    "        # RIGHT: PDF panel + Upload\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## üìö Danh s√°ch t√†i li·ªáu ƒë√£ n·∫°p\")\n",
    "            pdf_list = gr.Markdown(ui_refresh_pdf_list())\n",
    "\n",
    "            gr.Markdown(\"## ‚ûï N·∫°p th√™m PDF m·ªõi\")\n",
    "            uploader = gr.File(\n",
    "                label=\"Ch·ªçn file PDF\",\n",
    "                file_types=[\".pdf\"],\n",
    "                file_count=\"multiple\"\n",
    "            )\n",
    "            ingest_btn = gr.Button(\"N·∫°p & c·∫≠p nh·∫≠t kho\", variant=\"secondary\")\n",
    "            ingest_status = gr.Textbox(label=\"Tr·∫°ng th√°i\", lines=3, interactive=False)\n",
    "\n",
    "            ingest_btn.click(\n",
    "                fn=ui_upload_and_ingest,\n",
    "                inputs=uploader,\n",
    "                outputs=[ingest_status, pdf_list]\n",
    "            )\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a771cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a105dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
